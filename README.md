[![LICENSE](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sebasmos/RL-Curiosity-is-All-You-Need/blob/main/LICENSE)

# ğŸ§ DeepCuriosity

**Simple PyTorch implementation of PPO and Intrinsic Curiosity Module (ICM)** for fast training and visualization in [MuJoCo](https://github.com/google-deepmind/mujoco) environments like `Swimmer-v4`.

> âœ… **Quick demo?** Open [Demo.ipynb](https://github.com/sebasmos/DeepCuriosity/blob/main/Demo.ipynb) to try it out interactively.

---

## ğŸ”§ Whatâ€™s Inside

- âœ… PPO baseline (MLP actor-critic)
- ğŸ” PPO + Curiosity (ICM)
- ğŸ“Š Logs, reward plots, and training metrics
- ğŸï¸ Video generation of trained agents
- ğŸ—‚ï¸ Organized folder structure for easy experiments

---

## ğŸ“¦ Installation

```bash
conda create -n ppo_icm python=3.11.11 -y
conda activate ppo_icm

pip install -r requirements.txt
```

---

## ğŸš€ How to Run

### Train PPO:

```bash
python baseline.py
```

### Train PPO + ICM:

```bash
python curiosity.py 
```

---

## ğŸ“ˆ Visualize Results

Use the `plot.ipynb` notebook to:

- Compare PPO vs PPO+ICM
- Plot reward curves
- Render videos from saved episodes

---

## ğŸ—‚ï¸ Project & Output Structure

```bash
RL/
â”œâ”€â”€ baseline.py         # PPO training
â”œâ”€â”€ icm.py              # PPO + ICM training
â”œâ”€â”€ utils.py            # Video, plotting, loading helpers
â”œâ”€â”€ models.py           # Agents and ICM
â”œâ”€â”€ buffer.py           # PPO buffer
â”œâ”€â”€ plot.ipynb          # Notebook to view results
â”œâ”€â”€ configs/config.yaml # Training settings (Hydra)
â””â”€â”€ data/               # <== ALL training artefacts live here
    â”œâ”€â”€ raw_pytorch/    # Baseline PPO runs
    â”‚   â””â”€â”€ <env>/                 # e.g. Swimmer-v4
    â”‚       â””â”€â”€ <run>_<timestamp>/ # e.g. mlp_256_raw_pytorch_20250501_101010
    â”‚           â”œâ”€â”€ checkpoints/          # model_update_<n>_steps_<k>.pt
    â”‚           â”œâ”€â”€ logs/                 # metrics.csv per update
    â”‚           â”œâ”€â”€ episode_rewards.npy   # per-episode returns
    â”‚           â””â”€â”€ video_*.mp4           # optional rendered roll-outs
    â””â”€â”€ ppo_icm/       # PPO + ICM runs
        â””â”€â”€ <env>/
            â””â”€â”€ <run>_<timestamp>/        # same layout as above
                â”œâ”€â”€ checkpoints/
                â”œâ”€â”€ logs/
                â”œâ”€â”€ episode_rewards.npy
                â””â”€â”€ video_*.mp4
```

---

## ğŸ¤ Contribute or Reach Out

Feel free to fork, open issues, or submit PRs.  
If you have questions or want to collaborate, [reach out to me](https://github.com/sebasmos) â€” happy to chat!

---

### âš¡ Missing / To-Do

- [ ] Add new metrics
- [ ] dm_control for adding more joints
- [ ] Optional curiosity bonuses (RND, Disagreement, etc.)
- [ ] Add support for more MuJoCo environments (e.g. Hopper, HalfCheetah)
- [ ] Add TensorBoard logging
- [ ] Add WandB support
- [ ] Include policy evaluation script
- [ ] Add support for model saving/loading via CLI

---

### ğŸ“š References

- Inspired by [Curiosity-Driven Exploration](https://arxiv.org/abs/2109.08603)
- Historical foundation: [Artificial Curiosity since 1990](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html)
